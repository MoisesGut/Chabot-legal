import pandas as pd
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import chromadb

# ğŸ“Œ Cargar archivo de sentencias
archivo_excel = "/home/moi13/prueba_tecnica/sentencias_pasadas.xlsx"
casos_legales = pd.read_excel(archivo_excel)

# ğŸ“Œ Renombrar columnas
casos_legales.rename(columns={
    "Relevancia": "relevancia",
    "Providencia": "providencia",
    "Tipo": "tipo",
    "Fecha Sentencia": "fecha",
    "Sentencia": "sentencia",
    "Tema - subtema": "tema_subtema",
    "resuelve": "sentencia",
    "sintesis": "sintesis"
}, inplace=True)

# ğŸ“Œ Si "sentencia" no existe, usar "sintesis"
if "sentencia" not in casos_legales.columns:
    print("âš ï¸ La columna 'sentencia' no existe, se usarÃ¡ 'resuelve' en su lugar.")
    casos_legales["sentencia"] = casos_legales["sintesis"]

# ğŸ“Œ Eliminar filas vacÃ­as
casos_legales.dropna(subset=["sentencia"], inplace=True)

# ğŸ“Œ Cargar modelo Flan-T5
modelo_path = "/home/moi13/prueba_tecnica/flan-base"
tokenizer = AutoTokenizer.from_pretrained(modelo_path)
model = AutoModelForSeq2SeqLM.from_pretrained(modelo_path)

# ğŸ“Œ Configurar ChromaDB
client = chromadb.PersistentClient(path="/home/moi13/prueba_tecnica/chroma_db")
collection = client.get_or_create_collection(name="casos_legales")

# ğŸ“Œ Insertar documentos en ChromaDB
for idx, row in casos_legales.iterrows():
    collection.add(
        ids=[str(idx)],
        documents=[row["sentencia"]],
        metadatas=[{
            "tipo": row["tipo"],
            "tema": row["tema_subtema"],
            "fecha": str(row["fecha"])
        }]
    )

print("ğŸ“¥ Sentencias agregadas a la base de datos.")

# ğŸ“Œ Obtener contexto relevante
def obtener_contexto(pregunta):
    resultados = collection.query(query_texts=[pregunta], n_results=3)
    if resultados['documents']:
        return " ".join(resultados['documents'][0])
    return "No hay informaciÃ³n relevante en la base de datos."

# ğŸ“Œ Generar respuestas con IA
def responder_pregunta(pregunta):
    print(f"\nğŸ¯ Pregunta recibida: {pregunta}")  # ğŸ”¹ Imprime la pregunta antes de resolverla
    contexto = obtener_contexto(pregunta)

    # ğŸ“Œ Manejo de preguntas especÃ­ficas
    if "sentencias de 3 demandas" in pregunta.lower():
        sentencias = casos_legales.head(3)[["sentencia", "fecha"]]
        return "\n\n".join([f"ğŸ“Œ {row['sentencia']} (Fecha: {row['fecha']})" for _, row in sentencias.iterrows()])

    if "de quÃ© se trataron las 3 demandas" in pregunta.lower():
        descripciones = casos_legales.head(3)[["sintesis"]]
        return "\n\n".join([f"ğŸ“Œ {row['sintesis']}" for _, row in descripciones.iterrows()])

    if "sentencia del caso que habla de acoso escolar" in pregunta.lower():
        caso_acoso = casos_legales[casos_legales["tema_subtema"].str.contains("acoso escolar", case=False, na=False)]
        if not caso_acoso.empty:
            sentencia = caso_acoso.iloc[0]
            return f"ğŸ“Œ {sentencia['sentencia']} (Fecha: {sentencia['fecha']})"
        return "No se encontrÃ³ un caso sobre acoso escolar."

    if "detalle de la demanda relacionada con acoso escolar" in pregunta.lower():
        caso_acoso = casos_legales[casos_legales["tema_subtema"].str.contains("acoso escolar", case=False, na=False)]
        if not caso_acoso.empty:
            return f"ğŸ“Œ {caso_acoso.iloc[0]['sintesis']}"
        return "No se encontrÃ³ informaciÃ³n sobre acoso escolar."

    if "casos que hablan sobre el PIAR" in pregunta.lower():
        casos_piar = casos_legales[casos_legales["tema_subtema"].str.contains("PIAR", case=False, na=False)]
        if not casos_piar.empty:
            return "\n\n".join([f"ğŸ“Œ {row['sintesis']}" for _, row in casos_piar.iterrows()])
        return "No se encontraron casos sobre PIAR."

    # ğŸ“Œ Generar respuesta con Flan-T5
    prompt = f"""Responde de manera clara y sencilla:

    Pregunta: {pregunta}
    
    Contexto:
    {contexto}
    
    Respuesta:"""

    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
    outputs = model.generate(
        **inputs, 
        max_length=200,  # ğŸ”¹ Ahora la respuesta serÃ¡ mÃ¡s clara y sin cortes
        temperature=0.7,  # ğŸ”¹ Aumentamos la variabilidad de la respuesta
        top_p=0.95,  
        num_beams=3, 
        num_return_sequences=1, 
        do_sample=True  # ğŸ”¹ Permitimos sampling para mÃ¡s naturalidad
    )
    
    respuesta = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return respuesta

# ğŸ“Œ Prueba con preguntas de ejemplo
preguntas_ejemplo = [
    "Â¿CuÃ¡les son las sentencias de 3 demandas?",
    "Â¿De quÃ© se trataron las 3 demandas anteriores?",
    "Â¿CuÃ¡l fue la sentencia del caso que habla de acoso escolar?",
    "Â¿Diga el detalle de la demanda relacionada con acoso escolar?",
    "Â¿Existen casos que hablan sobre el PIAR, indique de quÃ© trataron los casos y cuÃ¡les fueron sus sentencias?"
]

for pregunta in preguntas_ejemplo:
    respuesta = responder_pregunta(pregunta)
    print(f"ğŸ¤– Respuesta generada: {respuesta}\n")
