# √çndice

- [Chatbot de Sentencias Judiciales üìú](#chatbot-de-sentencias-judiciales-)
  - [Caracter√≠sticas principales](#caracter√≠sticas-principales)
  - [Tecnolog√≠as utilizadas](#tecnolog√≠as-utilizadas)
- [Instalaci√≥n y configuraci√≥n](#instalaci√≥n-y-configuraci√≥n)
- [PRUEBA T√âCNICA](#prueba-t√©cnica)
- [Investigaci√≥n](#investigaci√≥n)
- [Desarrollo](#desarrollo)
  -[Preguntas](#preguntas)
- [Informe](#informe)
- [Supuestos](#supuestos)
- [Formas para resolver el caso y la opci√≥n tomada en esta prueba](#formas-para-resolver-el-caso-y-la-opci√≥n-tomada-en-esta-prueba)
- [Resultados del an√°lisis de datos y de lo modelos](#resultados-del-an√°lisis-de-datos-y-de-lo-modelos)
  - [Resultado An√°lisis de datos](#resultado-an√°lisis-de-datos)
  - [An√°lisis de sentencias por mes](#an√°lisis-de-sentencias-por-mes)
  - [Resultado del an√°lisis de los temas m√°s comunes](#resultado-del-an√°lisis-de-los-temas-m√°s-comunes)
  - [An√°lisis de la Nube de Palabras de los temas y s√≠ntesis de los casos](#an√°lisis-de-la-nube-de-palabras-de-los-temas-y-s√≠ntesis-de-los-casos)
- [Resultados del An√°lisis de los Modelos](#resultados-del-an√°lisis-de-los-modelos)
  - [Evaluaci√≥n del Rendimiento del Sistema](#evaluaci√≥n-del-rendimiento-del-sistema)
  - [An√°lisis de la Precisi√≥n y Calidad de las Respuestas](#an√°lisis-de-la-precisi√≥n-y-calidad-de-las-respuestas)
- [Futuros Ajustes o Mejoras](#futuros-ajustes-o-mejoras)
- [Apreciaciones y Comentarios del Caso](#apreciaciones-y-comentarios-del-caso)
- [Conclusi√≥n](#conclusi√≥n)
- [Referencias](#referencias)

# Chatbot de Sentencias Judiciales üìú  

Este chatbot est√° dise√±ado para responder preguntas sobre sentencias judiciales utilizando **FastAPI, ChromaDB y un modelo de lenguaje natural basado en transformers**. Su objetivo es proporcionar respuestas en **lenguaje claro y comprensible** para la mayor√≠a de las personas, sin tecnicismos legales complejos.

---

## Caracter√≠sticas principales  
* **B√∫squeda de sentencias por ID**  
* **Respuestas en lenguaje natural, sin tecnicismos**  
* **Memoria conversacional**: recuerda sentencias consultadas recientemente  
* **Base de datos vectorial con ChromaDB** para respuestas m√°s precisas  
* **Interfaz de usuario interactiva con React**  
* **Exposici√≥n del backend con Ngrok** para pruebas remotas  

---

## Tecnolog√≠as utilizadas  

| Tecnolog√≠a | Descripci√≥n |
|------------|------------|
| üêç **Python** | Lenguaje de programaci√≥n del backend |
| ‚ö° **FastAPI** | Framework para crear APIs r√°pidas |
| üîç **ChromaDB** | Base de datos vectorial para b√∫squeda eficiente |
| ü§ñ **Transformers (Hugging Face)** | Modelo mBART para respuestas en lenguaje natural |
| üé® **React** | Framework de frontend para la interfaz del chatbot |
| üåç **Ngrok** | Herramienta para exponer el backend de manera segura |

---

## Instalaci√≥n y configuraci√≥n  

### **IMPORTANTE**  
El presente reporitorio se uso para hacer una prueba tecnica en azure, por obvias razones no se comparte bases de datos, para compilar el codigo main.py es necesario tener descargado el modelo de IA BART-BASE-5 El cual esta disponible en el repositorio oficial de hugging face o puedes descargarlo directamente desde de tu codigo, adem√°s se tienen que instalar las librerias necesariasdisponibles en requeriments.txt, el proyecto consiste en el desarrollo de un back end en python "main.py", un desarrollo de front end "App.js" y "App.css" sin embargo si se cuenta con un equipo de computo con por lo menos 16gb ram y gpu puedes correr el modelo directamente desde tu maquina local usando el codigo "modelo.py", as√≠ mismo se realizo un analisis de los datos de la base "Analisis_Sentencias.ipynb" disponible en el repositorio actual.

:warning: Si se desea hacer pruebas de la aplicacion ingresar al siguiente link, la disponibilidad esta sujeta al servidor el cual se encuentra en una maquina virtual: chatbot-legal.ngrok.app :warning:
 
---

# PRUEBA T√âCNICA

**Caso de Negocio:** Asesor Legal para Consulta de Historia de Demandas

## Informe de Resultados

---

**Elaborado por:**  
Gutierrez Carmona Mois√©s

---

**Fecha de Entrega:** 04 de febrero de 2025

# Investigaci√≥n

**Modelos de IA Generativa**

Este tipo de modelos, sobre todo los basados en arquitecturas de Transformers, han sido importantes en el campo del Procesamiento del Lenguaje Natural. Estos modelos se entrenan a partir de grandes cantidades de datos y son capaces de generar respuestas coherentes y con contexto.

Mbart es una extensi√≥n del modelo BART que se ha desarrollado en trabajar en un contexto multiling√ºe. La versi√≥n usada en esta prueba denominada mBart-BASE-5 representa una variante optimizada para 5 idiomas, permitiendo generar textos en m√∫ltiples lenguajes con una alta calidad. Este modelo utiliza un proceso de preentrenamiento mediante eliminaci√≥n de ruido de los datos que permite aprender representaciones ling√º√≠sticas robustas. La elecci√≥n de este modelo para la prueba fue debido a que uno de los 5 idiomas en los que se especializa es el espa√±ol y es un modelo bastante ligero que no requiere mucha capacidad de computo. [1]

**Bases vectoriales** 

Son estructuras de datos que permiten el almacenamiento y la b√∫squeda de informaci√≥n a partir de la representaci√≥n num√©rica de los datos (vectores). Estas bases facilitan la b√∫squeda sem√°ntica.

Para esta prueba se uso Chroma DB, la cual esta dise√±ada para integrarse con modelos de embedding. Permitiendo indexar y recuperar informaci√≥n de forma eficiente, lo que hace eficiente la b√∫squeda sem√°ntica r√°pida y escalable. [3] 

**Limitaciones de un chabot.**

Dentro de esta prueba t√©cnica se percibieron las siguientes limitaciones:

Aunque los modelos han mejorado, comprender contextos complejos siguen siendo un desaf√≠o. Los chatbots pueden perder el hilo de la conversaci√≥n. Los sesgos y limitaciones de los datos de entrenamiento pueden llevar a respuestas inadecuadas o parciales. En los entornos que son constantemente din√°micos o con informaci√≥n muy especializada, los chatbots pueden carecer de la capacidad de adaptarse a preguntas nuevas sin un reentrenamiento. [2]

# Desarrollo

La siguiente prueba t√©cnica se llev√≥ a cabo en **Azure**, utilizando los siguientes recursos:  

- **M√°quina virtual (Standard D4as v6, 4 vCPU, 16 GiB de memoria):** Seleccionada para garantizar un rendimiento √≥ptimo del modelo de IA generativa, aprovechando su capacidad de c√≥mputo para procesar consultas de manera eficiente.
  ![image](https://github.com/user-attachments/assets/c46dfe9a-2e0c-406d-ba4c-22e4434ddfe3)
-	**File storage (Recursos Compartdios):):** Con la finalidad de compartir recursos con la maquina virtual, entre los que se encuentra los archivos necesario para el modelo de IA generativa.
  ![image](https://github.com/user-attachments/assets/c88ab392-98c6-43ca-9d4e-cf1d67f68a20)
- Se cre√≥ un grupo de recursos para contene todo lo que ocuparemos.
 	![image](https://github.com/user-attachments/assets/ad90963b-d167-476d-a60f-2c3b88f8e4ec)
- Se cre√≥ un entorno optimizado para garantizar su correcta ejecuci√≥n, asegurando la compatibilidad con los recursos y dependencias necesarias.
  ![image](https://github.com/user-attachments/assets/6a651270-c768-46af-a8d6-44a4610cc546)
- En la siguiente carpeta tenemos almacenado el modelo de ia generativa y la base de chroma db, as√≠ como la base de datos que ocuparemos para entrenar el modelo.
  ![image](https://github.com/user-attachments/assets/a514925d-5aa2-49cf-9084-623a809413c8)
  ![image](https://github.com/user-attachments/assets/75fc8b46-0515-4aa3-891f-ec6cd7020136)
- Este es el directorio principal del proyecto, donde se tiene main.py el cual es el back end desarrollado en python
  ![image](https://github.com/user-attachments/assets/786518c6-9637-4422-8c6b-8318f0ed5a82)
- Dentro de la carpeta frontend tenemos toda la implementacion del front.
  ![image](https://github.com/user-attachments/assets/cf9c6020-ad4d-458a-995d-854abbacc86e)
- La aplicaci√≥n desarrollada se encuentra principalmente en App.js y App.css.
- Una vez teniendo el desarrollo del back en main.py, desplegamos el servidor en el puerto 8000
  ![image](https://github.com/user-attachments/assets/8eaae925-faf1-4c8e-bf1f-2b0027032f1e)
- Se levanta el servidor:
  ![image](https://github.com/user-attachments/assets/ada6f7e5-9021-4585-aecd-10732fd8f80c)
- Posteriormente con ngrok exponemos el puerto para redes publicas.
  ![image](https://github.com/user-attachments/assets/fcbfa0c2-adc7-4db3-a69b-adf6a12aa069)
- Desplegamos el front end en el puerto 3000.
  ![image](https://github.com/user-attachments/assets/39fc8c8b-a523-4d09-aa23-34a22f7eb9f8)
- Y exponemos el puerto con ngrok nuevamente.
  ![image](https://github.com/user-attachments/assets/91cd0fcc-941e-4e13-8fdb-485b2b036662)
- Verificamos que el servidor del back end este funcionando correctamente.
  ![image](https://github.com/user-attachments/assets/d66439f6-c72a-489d-aaf8-6ebdee95175e)
- Verificamos que el servidor del front end este correcto.
  ![image](https://github.com/user-attachments/assets/37fbcbce-719c-4a99-8aec-44853f6d7998
  
 ##  Preguntas##
 
- Se hace la pregunta el servidor recibe la pregunta:
  ![image](https://github.com/user-attachments/assets/30fa7599-f491-46c0-99a5-39436ffc53d6)
- El servidor del back end recibe la pregunta:
  ![image](https://github.com/user-attachments/assets/aebb6156-7dcf-4f4b-bfd7-d0eb1f5fd10c)
- El servidor recibe la pregunta relacionada con la anterior.
  ![image](https://github.com/user-attachments/assets/b9285533-f665-4c81-8160-211e264639fc)
- El servidor recibe la pregunta y se implementa un sistema de memoria para que el back end guarde los ids de los casos que le dimos en la pregunta anterior.
  ![image](https://github.com/user-attachments/assets/533b0d77-fce3-4632-849b-c7c094d075f2)
- NOTA: Estas preguntas tienen una baja latencia porque ya se tienen mapeados lo casos en funciones dentro de python.
- Se envia la pregunta desde el back end.
  ![image](https://github.com/user-attachments/assets/d5a5e62b-c5be-4444-846f-d2f1e3a324f5)
- Se agrega una funcion para que el front end muestre mientras recibe la respuesta del back end, en estos casos siguientes la latencia es mayor ya que no tenemos mapeados al 100% estos casos.
  ![image](https://github.com/user-attachments/assets/75911199-d4fa-478d-a9c9-3273a0c6fb44)
- El servidor recibe la respuesta, aqu√≠ dejamos que el modelo haga su trabajo, es una pregunta que no conoce pero trata de responder.
  ![image](https://github.com/user-attachments/assets/ca998a9e-a0d6-40b9-b45c-2017d144e09b)
- Se envia la pregunta:
  ![image](https://github.com/user-attachments/assets/cdd1cfa5-5343-47a7-b022-0e1db187c0fc)
  ![image](https://github.com/user-attachments/assets/37cf594c-0ce0-45f3-940b-ba68013e05f3)
  ![image](https://github.com/user-attachments/assets/f09fdc05-3f8a-4311-945b-3c74633f44c8)
- El servidor recibe la pregunta sin conocerla antes:
  ![image](https://github.com/user-attachments/assets/10085417-2655-4e19-b607-890052758339)
  ![image](https://github.com/user-attachments/assets/a7ca1b17-25d5-4556-8e29-993891394778)
  ![image](https://github.com/user-attachments/assets/df45b8d6-0664-4ad8-9afb-40808519cc56)
Como podemos ver al aumentar la longitud de la pregunta disminuye el rendimiento del modelo de IA generativa, ademas de que no es capaz de contestar la pregunta en concreto.

# Informe

Dentro de un consultorio legal se busca optimizar la b√∫squeda de antecedentes y sentencias de los diversos casos que se les presenta, principalmente referente a temas de redes sociales. En la actualidad, los abogados hacen esta tarea de forma manual en una hoja de Excel para proporcionar detalles a sus clientes sobre las posibles demandas y penas que podr√≠an estar enfrentando.

Teniendo en cuenta esto, la presente prueba plantea el uso de Inteligencia Artificial Generativa, de manera que el sistema pueda responder de forma automatizada y en un lenguaje coloquial, las preguntas m√°s frecuentes sobre la historia de demandas y sus sentencias.

En esta prueba de concepto, se requiere que la herramienta responda a:
- Las sentencias de tres demandas espec√≠ficas.
- El contexto de esas mismas tres demandas.
- Informaci√≥n detallada sobre un caso de acoso escolar (sentencia y detalles).
- La existencia (y detalle) de casos sobre el PIAR.

# Supuestos

1. **Disponibilidad y calidad de datos**  
   - Se toma por hecho que la base de datos con las demandas y sus sentencias est√° completa, correctamente etiquetada y organizada para extraer informaci√≥n relevante.

2. **Uso de un modelo de IA generativa**  
   - Se asume que el modelo seleccionado (BART-BASE-5) puede interpretar de forma adecuada las descripciones de los casos y responder con frases coherentes y contextualizadas.

3. **Lenguaje coloquial**  
   - Se asume que la herramienta debe reescribir la informaci√≥n en t√©rminos sencillos, sin tecnicismos, sin perder la precisi√≥n en lo legal, pero con un vocabulario que la mayor√≠a de la gente pueda comprender.

4. **Limitaciones**  
   - Al ser una implementaci√≥n inicial, se requiere un volumen de datos m√°s robusto y variado para mejorar la precisi√≥n del modelo.

# Formas para resolver el caso y la opci√≥n tomada en esta prueba

Existen diferentes tipos de abordar problemas en cuanto a datos se refiere.

- **Uso de bases de datos relacionales para realizar b√∫squedas**
  - Consiste en realizar consultas a trav√©s de la base de datos usando palabras claves.
  - Con esta soluci√≥n podemos hacer filtros, sin embargo, no ofrece explicaciones usando PLN (Procesamiento del lenguaje natural) ni la capacidad de resumir.

- **Sistemas de b√∫squeda sem√°ntica**
  - Utiliza un motor de b√∫squeda basado en la indexaci√≥n de documentos por su contenido sem√°ntico.
  - Se puede localizar r√°pidamente la informaci√≥n relevante y puede integrarse con modelos generativos para responder de forma m√°s natural.

- **Implementaci√≥n de un chatbot con IA generativa**
  - Se integra un modelo generativo con una base de datos vectorial donde se pueden almacenar los datos.
  - El agente de IA busca la informaci√≥n en la base vectorial, posteriormente, el modelo de IA generativa construye la respuesta en lenguaje coloquial.

Para el caso de uso de esta prueba, se opt√≥ por la tercera opci√≥n.

En la gesti√≥n de embeddings y la recuperaci√≥n eficiente de documentos, se utiliz√≥ **Chroma DB**, una base de datos vectorial dise√±ada para almacenamiento y consulta de datos sem√°nticos en aplicaciones de IA generativa (Chroma Team, 2023). [3]

- Se cre√≥ un prototipo de chatbot que fuera capaz de:
  - Vectorizar los textos de las demandas y sentencias para facilitar la b√∫squeda sem√°ntica.
  - Realizar consultas a la base de datos vectorial con el fin de recuperar los documentos m√°s relevantes.
  - Generar la respuesta con un modelo de lenguaje que simplifica y contextualiza la informaci√≥n. Seg√∫n OpenAI (2023), la incorporaci√≥n de modelos de gran tama√±o (Large Language Models) mejora significativamente la generaci√≥n de texto coherente. [4]
 
# Resultados del an√°lisis de datos y de lo modelos
- **Calidad de b√∫squeda sem√°ntica:**  
  Al indexar el contenido de los casos en una base de datos vectorial, se increment√≥ la precisi√≥n en la identificaci√≥n de demandas similares o relacionadas.

- **Respuestas en lenguaje natural:**  
  El modelo generativo demostr√≥ ser capaz de reformular las sentencias y res√∫menes en un lenguaje muy natural, tal como se requer√≠a. La librer√≠a Transformers de Hugging Face facilita la integraci√≥n de modelos de lenguaje de vanguardia en m√∫ltiples tareas de procesamiento de lenguaje natural (Hugging Face, n.d.). [5]

- **Desempe√±o:**  
  Para un conjunto de datos de tama√±o modero, la latencia con la que responde fue aceptable. Sin embargo, en situaciones con m√°s volumen de datos, ser√≠a necesario optimizar tanto la indexaci√≥n como la generaci√≥n de texto para mantener la eficacia de las respuestas.

## Resultado An√°lisis de datos ##

Se realizo un an√°lisis de datos el cual esta dentro del repositorio llamado "Analisis_Sentencias.ipynb", los resultados fueron los siguientes: 

  ### An√°lisis de las sentencias recibidas por a√±o

La gr√°fica refleja un crecimiento en el n√∫mero de sentencias desde 2019, con un aumento sostenido hasta alcanzar su punto m√°s alto en 2023, superando las 80 resoluciones. En 2024 se observa una ligera disminuci√≥n, aunque el n√∫mero de sentencias sigue siendo significativamente mayor en comparaci√≥n con los a√±os previos a 2022. A pesar de la ca√≠da en 2024, el nivel de sentencias sigue siendo elevado, lo que podr√≠a indicar una estabilizaci√≥n en la capacidad del sistema interno de los abogados para gestionar demandas.

![image](https://github.com/user-attachments/assets/831d08b3-4e0f-4061-a58b-996a658e8729)

# An√°lisis de sentencias por mes

La gr√°fica muestra un aumento en las sentencias gestionadas por el despacho desde 2019, con un pico en 2023 y estabilizaci√≥n en 2024. Los meses con m√°s actividad son agosto, septiembre y octubre, mientras que enero y febrero presentan menos casos. El crecimiento desde 2022 sugiere una mayor demanda de servicios o mejoras en los procesos internos.

![image](https://github.com/user-attachments/assets/90ffed15-7f09-4e88-91b6-ed9e7e1daf60)

# Resultado del an√°lis de los temas mas comunes

La mayor√≠a de los casos gestionados por el despacho est√°n relacionados con la competencia de la jurisdicci√≥n contencioso-administrativa y acciones de tutela. Sin embargo, la gran proporci√≥n de sentencias clasificadas como desconocido lo que que podr√≠a ser √∫til mejorar la categorizaci√≥n de los casos para obtener un an√°lisis m√°s preciso de las tendencias legales.

![image](https://github.com/user-attachments/assets/245cf2b7-23e4-4495-a6de-8436bfb98a8b)

# An√°lisis de la Nube de Palabras de los temas y sintesis de los casos

La nube de palabras muestra que los casos del despacho se enfocan en derechos fundamentales, regulaci√≥n estatal y conflictos digitales, destacando temas como emergencia econ√≥mica, libertad de expresi√≥n y redes sociales.

![image](https://github.com/user-attachments/assets/ce116bde-6ee0-4442-af26-12e9dec656bd)

![image](https://github.com/user-attachments/assets/677e8325-398c-43cd-be7a-ce04a1803b2c)

# Resultados del An√°lisis de los Modelos

## Evaluaci√≥n del Rendimiento del Sistema

Durante la prueba t√©cnica, se analizaron distintos aspectos del sistema, desde la capacidad de procesamiento hasta la latencia en la obtenci√≥n de respuestas por parte del modelo de IA generativa. A continuaci√≥n, se presentan los principales hallazgos:

- **Desempe√±o de la M√°quina Virtual:**  
  La configuraci√≥n utilizada en Azure (Standard D4as v6 con 4 vCPU y 16 GiB de RAM) permiti√≥ un rendimiento estable del modelo de IA generativa. Sin embargo, cuando se realizaron consultas m√°s extensas, se observ√≥ un incremento en la latencia de respuesta.

- **Almacenamiento y Disponibilidad de Datos:**  
  La integraci√≥n de ChromaDB para la indexaci√≥n y recuperaci√≥n de sentencias almacenadas permiti√≥ optimizar el acceso a la informaci√≥n. No obstante, en algunos casos espec√≠ficos, el sistema no encontr√≥ coincidencias exactas, lo que afect√≥ la calidad de las respuestas.

## An√°lisis de la Precisi√≥n y Calidad de las Respuestas

### **Consultas con IDs espec√≠ficos de casos legales**  
- Cuando se pregunt√≥ por sentencias espec√≠ficas (ej. "¬øCu√°l fue la sentencia de las siguientes demandas: T-185/22, T-351/21, T-351/22?"), el sistema fue capaz de extraer correctamente la informaci√≥n almacenada en ChromaDB.  
- Sin embargo, en casos donde no exist√≠a una coincidencia exacta, el modelo generaba respuestas gen√©ricas, indicando que no encontr√≥ informaci√≥n relevante.

### **Consultas de seguimiento**  
- Se implement√≥ un sistema de memoria que permite recordar los IDs consultados en la pregunta anterior.  
- Gracias a esto, cuando se le pregunt√≥ "¬øDe qu√© se trataron las 3 demandas anteriores?", el sistema fue capaz de recuperar los detalles correspondientes a las sentencias previamente mencionadas.

### **Consultas de contexto general**  
- Preguntas m√°s amplias, como "¬øDiga el detalle de la demanda relacionada con acoso escolar?", fueron procesadas de manera efectiva cuando la informaci√≥n estaba disponible en la base de datos.  
- En algunos casos, el modelo gener√≥ respuestas excesivamente detalladas, por lo que se ajust√≥ la configuraci√≥n para priorizar respuestas m√°s concisas y naturales.

## Impacto del Tama√±o de la Pregunta en el Rendimiento

Se identific√≥ que la latencia del sistema aumentaba cuando las preguntas conten√≠an m√∫ltiples IDs o eran demasiado extensas. Esto se debe a:

- **Mayor cantidad de consultas a ChromaDB:** Cuando se requiere extraer informaci√≥n sobre m√∫ltiples sentencias en una sola consulta, el sistema necesita realizar varias b√∫squedas simult√°neas.
- **Procesamiento del modelo de IA generativa:** Preguntas m√°s largas implican un procesamiento m√°s complejo en el modelo, lo que puede generar respuestas menos precisas o incoherentes.

Para mitigar este problema, se implementaron las siguientes soluciones:

- **L√≠mite de longitud en las respuestas:** Se ajust√≥ el modelo para proporcionar respuestas m√°s cortas y directas, evitando la sobrecarga de informaci√≥n innecesaria.
- **Preprocesamiento de preguntas:** Se mejor√≥ la extracci√≥n de IDs de sentencias para optimizar las b√∫squedas en ChromaDB.
- **Mensajes en el frontend:** Se agreg√≥ un indicador de Escribiendo... mientras el backend procesa la consulta, mejorando la experiencia del usuario.


Los resultados obtenidos demuestran que la integraci√≥n de ChromaDB y un modelo de IA generativa es una soluci√≥n viable para responder preguntas sobre sentencias legales de manera eficiente. Sin embargo, se identificaron √°reas de mejora, como:

- **Optimizaci√≥n del tiempo de respuesta** para consultas extensas.
- **Mejora en la coherencia de las respuestas** cuando no hay coincidencias exactas en la base de datos.
- **Mayor control sobre la longitud y claridad** de las respuestas generadas

# Futuros Ajustes o Mejoras

- **Ampliar la base de datos**  
  Incluir m√°s casos de demandas para que el modelo pueda aprender a dar respuestas m√°s contextualizadas.

- **Filtrar informaci√≥n**  
  A√±adir un mecanismo para que datos sensibles de los clientes puedan ser an√≥nimos antes de presentar la informaci√≥n.

- **Entrenamiento**  
  Ajustar el modelo con m√°s ejemplos donde se use lenguaje legal y sus equivalentes en lenguaje coloquial para mejorar la precisi√≥n de las respuestas.

- **Sistema**  
  Implementar un sistema de retroalimentaci√≥n que permita a los abogados calificar o validar las respuestas generadas con la finalidad de mejorar la exactitud del modelo.

---

## Apreciaciones y Comentarios del Caso  

La implementaci√≥n de IA generativa reduce el tiempo de b√∫squeda y proporciona una explicaci√≥n m√°s amigable para el usuario final. Adem√°s, este enfoque presenta escalabilidad y rentabilidad, pues, al entrenar el modelo con m√°s datos, tiene el potencial de convertirse en una herramienta que facilite las tareas diarias tanto de los abogados como de los usuarios. Para finalizar, esta herramienta tiene la capacidad de expandirse, ya que, si bien su uso actual es para temas legales, la metodolog√≠a basada en b√∫squeda sem√°ntica y modelos de IA generativa resulta aplicable a numerosos campos de estudio.

---

## Conclusi√≥n  

En este informe se describe la soluci√≥n propuesta para automatizar las consultas del historial de demandas y sentencias mediante el uso de IA generativa. La prueba de concepto demuestra la viabilidad de aplicar t√©cnicas de b√∫squeda sem√°ntica y modelos generativos, en especial para agilizar trabajos repetitivos como consultas y proporcionar respuestas en lenguaje coloquial. A medida que se haga m√°s robusta la base de datos y se realicen diversos ajustes en el modelo, el resultado podr√≠a ser una mayor precisi√≥n, haciendo que la herramienta sea a√∫n m√°s √∫til.

---

## Referencias  

[1] Hugging Face AI. (s.f.). mBART-50 one to many multilingual machine translation. https://huggingface.co/facebook/mbart-large-50-one-to-many-mmt

[2] IBM. (s.f.). Chatbot Limitations: Challenges and Considerations. https://www.ibm.com/mx-es/topics/chatbots

[3] Chroma Team. (2023). *Chroma Documentation*. [https://docs.trychroma.com](https://docs.trychroma.com)  

[4] OpenAI. (2023). *ChatGPT: Introducing a new language model*. OpenAI. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)  

[5] Hugging Face. (n.d.). *Transformers: State-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX*. [https://huggingface.co/docs/transformers](https://huggingface.co/docs/transformers)  









